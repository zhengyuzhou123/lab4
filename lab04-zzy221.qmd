---
title: "Job Postings Data Analysis"
author: "Zhengyu Zhou"
format:
  html:
    toc: true
    theme: cosmo
  docx:
    toc: true
execute:
  echo: true
  eval: true
  freeze: auto
jupyter: python3
---

---
jupyter: python3
---

```{python}
from pyspark.sql import SparkSession

# Start a Spark session
spark = SparkSession.builder.appName("JobPostingsAnalysis").getOrCreate()

# Load the CSV file into a Spark DataFrame
df = spark.read.option("header", "true").option("inferSchema", "true").option("multiLine","true").option("escape", "\"").csv("data/lightcast_job_postings.csv")

# Show schema
df.printSchema()
```

```{python}
pdf = df.limit(5000).toPandas()

print("Dataset Info:")
pdf.info()

print("First 5 Rows:")
print(pdf.head())

print("Missing Values:")
print(pdf.isnull().sum())
```

```{python}
# Delete duplicate lines
df_cleaned = df.dropDuplicates()

# Check the number of rows after cleaning
original_count = df.count()
cleaned_count = df_cleaned.count()

print(f"removed duplicates: {original_count - cleaned_count} rows deleted.")
print(f"Remaining rows: {cleaned_count}")
```

```{python}
# Handle Missing Values

from pyspark.sql import functions as F

# Convert to numeric type
df = df.withColumn("SALARY", F.col("SALARY").cast("double"))

# Calculate the median and fill it in missing values
median_salary = df.approxQuantile("SALARY", [0.5], 0.01)[0]
df = df.fillna({"SALARY": median_salary})

print(f"Filled missing salaries with median: {median_salary}")
```

```{python}
from pyspark.sql import functions as F

#The date of conversion
date_cols = ["LAST_UPDATED_DATE","POSTED", "EXPIRED",]

for c in date_cols:
    df = df.withColumn(
        c,
        F.when(F.col(c).rlike(r"^\d{1,2}/\d{1,2}/\d{4}$"), F.to_date(F.col(c), "M/d/yyyy")).otherwise(None)
    )
    print(f"Safely converted {c} to date type (invalid values set to NULL).")

df.select(date_cols).show(5)
df.printSchema()
```

```{python}
#Count Total Job Postings

total_postings = df.count()
print(f"Total Job Postings: {total_postings}")
```

```{python}
#Find Top 5 Job Titles

from pyspark.sql import functions as F
top_job_titles = df.groupBy("TITLE_RAW").count().orderBy(F.desc("count")).limit(5)
top_job_titles.show()
```

```{python}
#Calculate Average Salary by Job Type
avg_salary_by_job_type = df.groupBy("TITLE_RAW").agg(F.avg("SALARY").alias("Average_Salary")).orderBy(F.desc("Average_Salary"))
avg_salary_by_job_type.show()
```

```{python}
#Find the State with the Most Job Postings

top_states = df.groupBy("STATE_NAME").count().orderBy(F.desc("count")).limit(5)
top_states.show()
```


